# Titanic - studies on classification

In this chapter we will turn our focus to classification techniques. The data set used will be the Kaggle data set *Titanic*.

The first section will focus on data collection and data wrangling. The second section has its focus on EDA. In the third and last section we will study the following classification models and how to optimize them for the dataset: Logistic Regression, LDA, kNN, SVM, XGBoost and lightGBM.

The evaluation metric will be the Kaggle score for each model.

## Data collection and cleaning

The data was downloaded from (Kaggle)[https://www.kaggle.com/c/titanic]. The data set is made up of two parts. Part 1 is a training set for supervised learning. Part 2 is a test set, where the outcome column has been removed.

We will begin by loading the two data files and have a look at the data types of each column:
```{r}
titanic_train <- read_csv("./titanic_train.csv")
titanic_test <- read_csv("./titanic_test.csv")
```
We see that the training data is comprised of 891 observation of 12 features, whereas the test data is comprised of 418 observations of 11 features. 

We have 5 features coded as charaters: Name, Sex, Ticket, Cabin, Embarked.

We have 6 numeric features: PassengerId, Pclass, Age, SibSp. Parch, Fare.

Let us first see if we need to deal with missing values:
```{r}
titanic_train %>% 
  select(which(colSums(is.na(.))>0)) %>% 
  summarise_all(~ sum(is.na(.)))
```
We see that most of the cabin data is missing. We will not deal with these for now, but drop the column, when doing the actual modelling later. 

For embarked we will replace the missing values with the mode. We start by determining the mode:
```{r}
titanic_train %>% group_by(Embarked) %>% count() %>% arrange(desc(n))
```

Then we replace the NAs by "S":
```{r}
titanic_train <- titanic_train %>% mutate(Embarked = replace(Embarked,is.na(Embarked),"S"))
```

Next, let's have a look at the distribution of age:
```{r}
titanic_train %>% count(Age_interval = cut_interval(Age,n=6))
```

We get see that Age seems right skewed. To verify this we calculate the skewness using the {moments} package:
```{r}
titanic_train %>% select(Age) %>% filter(!is.na(.)) %>% summarise(Skewness = moments::skewness(.))
```
With the right skewed age data we choose to impute the missing values in Age column with the median.
```{r}
titanic_train <- titanic_train %>% mutate(Age = replace(Age,is.na(Age),median(Age,na.rm=TRUE)))
```

With this we conclude the data collection and preparation process and move on to the EDA.

## EDA

Lets begin by looking at the prior distrubution of survivals:
```{r}
titanic_train %>% 
  mutate(Survived = as_factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "Died"="0","Survived"="1")) %>%  
  ggplot(aes(x=Survived,fill=Survived)) +
    geom_bar(show.legend = FALSE) + 
    labs(x="Survival Status", y="Passenger Count", title = "Distribution of passenger status")
```
We see that roughly 3 of 5 died in the Titanic disaster. We now turn our attention to finding reasonable correlations in the data. A place to start would be a correlation diagram. We can choose to look only at the numerical features in the data set:
```{r, warning=FALSE}
titanic_train %>% 
  select(which(sapply(.,is.numeric))) %>% 
  correlate(diagonal = 1,quiet=TRUE) %>% 
  rplot() +
    guides(x=guide_axis(angle=90))
```
Alternatively we can one hot encode the relevant categorical variables and include them as well:
```{r}
recipe(Survived ~ .,data=titanic_train) %>% 
  update_role(PassengerId,Name,Ticket,Cabin,new_role="ID") %>% 
  step_dummy(all_nominal_predictors(),one_hot=TRUE) %>% 
  prep(retain=TRUE) %>% 
  juice() %>% 
  select(which(sapply(.,is.numeric))) %>% 
  correlate(diagonal = 1) %>% 
  rplot() + theme(legend.position = "top") +
    guides(x = guide_axis(angle=90))
```
From the correlation plot we see that Fare and PClass are moderately correlated. This can become an issue later on in the modelling process. We also see that Survived primarily is correlated to Sex and PClass. Lets investigate these further.

First let us have a look at how survived is related to sex:
```{r}
titanic_train %>% 
  mutate(Survived = as_factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "Died"="0","Survived"="1")) %>% 
  ggplot(aes(x=Sex, fill=Survived)) +
    geom_bar(position="dodge")
```
As expected the two variables are correlated. A higher proportion of women survived compared to men.

Next let us have a look at the Pclass vs Survived:
```{r}
titanic_train %>% 
  mutate(Survived = as_factor(Survived)) %>%
  mutate(Pclass = as_factor(Pclass)) %>% 
  mutate(Survived = fct_recode(Survived, "Died"="0","Survived"="1")) %>% 
  ggplot(aes(x=Pclass, fill=Survived)) +
    geom_bar(position="dodge")
```
From this we see that a 1st class passenger was more likely to survive than a 3rd class passenger.

Finally we can have a look at the fare distributions of the passengers:
```{r}
titanic_train %>% 
  mutate(Survived = as_factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "Died"="0","Survived"="1")) %>%
  ggplot(aes(x=Fare,color=Survived)) +
    stat_density(geom="line")
```
We see that among those that died, there is a higher proportion of passengers with a low fare. This is easily explained from the higher proportion of 3rd class passengers among those who died.

## Modelling

In this section we will develop several models to predict if a passenger will survive or not. Different model classes have different requirements to feature engineering, so we will develop several preprocessing recipes and choose the ones that performs best with respect to accuracy and ROC.

Since some models are able to deal well with NAs we reload the original dataset with missing values for modelling purposes:
```{r}
titanic_train_model <- read_csv("./titanic_train.csv")
```
For the classification models we need the outcome to be a factor variable:
```{r}
titanic_train_model <- titanic_train_model %>% mutate(Survived = as_factor(Survived))
```

Let us have a look at the skewness of the different numerical variables to decide upon which kind of imputation to choose:
```{r}
titanic_train_model %>% 
  select(which(sapply(.,is.numeric)),-PassengerId) %>%
  summarise_all(~ moments::skewness(.))
```
We see that all variables are skewed, so we impute them using the median.

All models use the same formula and need to have the same columns as ID columns, so we begin by making a base recipe for the preprocessing:
```{r}
titanic_baserec <- 
  recipe(Survived ~ ., data=titanic_train_model) %>% 
  step_rm(Ticket,Cabin,Name) %>% 
  update_role(PassengerId,new_role="ID")
```

To tune the different models we will use 10-fold cross validation. For this purpose we define the folds:
```{r}
set.seed(123)
titanic_folds <- vfold_cv(titanic_train_model,strata = Survived)
```

### Logistic Regression

Logistic regression relies on the features being independent. Moreover it doesn't do
well with missing values and factors with small frequencies. This gives us the following
recipe for logistic regression:
```{r}
titanic_logit_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())
```

For decorrelation we can choose between PCA and removing correlated variables. Additionally we can decide to normalize the numeric predictors or not. If we do, we use Yeo-Johnson transformation. This gives us the following final recipes for logistic regression:
```{r}
titanic_logit_pca_nonorm <-
  titanic_logit_rec %>% 
  step_pca(all_predictors())

titanic_logit_decor_nonorm <-
  titanic_logit_rec %>% 
  step_corr(all_predictors())

titanic_logit_pca_norm <-
  titanic_logit_rec %>% 
  step_pca(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors())

titanic_logit_decor_norm <-
  titanic_logit_rec %>% 
  step_corr(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors())
```

For models we can use a regularized or un-regularized logistic regression model. For the regularized model we will using 10-fold cross validation tuning to choose the best regularization parameters.
```{r}
titanic_logit_mod <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm")

titanic_reg_logit_mod <- logistic_reg(penalty=tune(),mixture=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")
```

Now we can define the workflow set for the different models and recipes:
```{r}
titanic_logit_wfs <- 
  workflow_set(
    preproc = list(DeCorNoNorm = titanic_logit_decor_nonorm, 
                   DeCorNorm = titanic_logit_decor_norm,
                   PCANoNorm = titanic_logit_pca_nonorm,
                   PCANorm = titanic_logit_pca_norm),
    models = list(regularized = titanic_reg_logit_mod,
                  unregularized =titanic_logit_mod)
  )
```

For tuning the regularized model we define the following tuning grid:
```{r}
titanic_logit_grid <- grid_regular(penalty(),mixture(),levels=c(5,2)) 
```

We are now ready to fit at tune the various models:
```{r}
titanic_logit_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_logit_results <- 
  titanic_logit_wfs %>% 
  workflow_map(
    seed = 1234,
    resamples = titanic_folds,
    grid = titanic_logit_grid,
    control = titanic_logit_control, 
    metrics = metric_set(accuracy,roc_auc),
    verbose = FALSE
  )
```

Let us have a plot of the fitted models and workflows:
```{r}
autoplot(titanic_logit_results,rank_metric="accuracy",select_best = TRUE) +
  geom_text(aes(y=0.45,label= wflow_id), angle=90) +
  lims(y=c(0.2,0.9)) +
  theme(legend.position = "none")
```
We see that using PCA to decorrelate features is performing worse than simple removal of correlated variables. Moreover normalizing doens't seem to increase performance and accuracy and ROC agree well when determining best model and workflow.

Let us find the best tuning parameters:
```{r}
titanic_logit_best_parm <-
   titanic_logit_results %>% 
   extract_workflow_set_result("DeCorNoNorm_regularized") %>% 
   select_best(metric="accuracy")
titanic_logit_best_parm
```
So the best option is Lasso regression with a slight penalty of 1.0e-10.
```{r}
titanic_logit_best_wf <- titanic_logit_results %>% 
  extract_workflow("DeCorNoNorm_regularized") %>% 
  finalize_workflow(titanic_logit_best_parm)

titanic_logit_best_mod <- fit(titanic_logit_best_wf,titanic_train_model)

titanic_logit_pred <- predict(titanic_logit_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_logit_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_logit_pred.csv")
```

The Kaggle score (which established the baseline for this chapter), is 0.76315. The accuracy is as expected slightly lower than for the training data, but a very respectable result for a linear model.

Lets us collect the results in a Tibble, which we can use at the end of the chapter:
```{r}
results_table <- tibble(Model = c("L2-penalized Logistic Regression"), Score = "0.76315")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

### LDA

The linear discriminant classifier (LDA-C) is mostly included for reference. In general it is not expected to perform as well as the penalized logistic regression.

Pre-processing for the LDA is almost the same as for logistic regression. We need to remove zero variance variables as well as ID variables. Moreover we need to one-hot 
encode factors and and de-corrolate features. Finally LDA might benefit from normalization of variables, in which case we use Yeo Johnson transformation.

This leaves us with the following preprocessing recipes:
```{r}
titanic_lda_nonorm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())

titanic_lda_norm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors())

titanic_lda_decor_nonorm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_corr(all_predictors())

titanic_lda_decor_norm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_corr(all_predictors())

titanic_lda_pca_nonorm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_pca(all_predictors())

titanic_lda_pca_norm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_pca(all_predictors())
```

This section is temporarily aborted due to problems with {Discrim} package.

### kNN

With no included preprocessing measures in the algorithm, the kNN model relies on heavy preprocessing steps including one hot encoding of dummy variables, removing zero variance variables, imputing NAs and normalizing variables. Decorrelation might improve importance. This gives us three different workflows:
```{r}
titanic_knn_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_best_normalize(all_predictors())

titanic_knn_decor_rec <- 
  titanic_knn_rec %>% 
  step_corr(all_predictors())

titanic_knn_pca_rec <-
  titanic_knn_rec %>% 
  step_pca(all_predictors())
```

We can now define the kNN model
```{r}
titanic_knn_mod <- nearest_neighbor(neighbors= tune(), weight_func=tune(), dist_power = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")
```

Next we set up the grid for the grid search. We will try 10 levels of n, 9 different weight functions and 4 different distance metrics:

```{r knn_grid}
titanic_knn_grid <- grid_regular(neighbors(),weight_func(),dist_power(),levels=c(10,9,4))
```

Now we can define the workflow set for the different models and recipes:
```{r}
titanic_knn_wfs <- 
  workflow_set(
    preproc = list(NoDecor = titanic_knn_rec, 
                   Decor = titanic_knn_decor_rec,
                   PCA = titanic_knn_pca_rec),
    models = list(kNN = titanic_knn_mod)
  )
```

We are now ready to fit at tune the various models:
```{r}
titanic_knn_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_knn_results <- 
  titanic_knn_wfs %>% 
  workflow_map(
    seed = 1234,
    resamples = titanic_folds,
    grid = titanic_knn_grid,
    control = titanic_knn_control, 
    metrics = metric_set(accuracy,roc_auc),
    verbose = FALSE
  )
```
Let us have a plot of the fitted models and workflows:
```{r}
autoplot(titanic_knn_results,rank_metric="accuracy",select_best = TRUE) +
  geom_text(aes(y=0.45,label= wflow_id), angle=90) +
  lims(y=c(0.2,0.9)) +
  theme(legend.position = "none")
```
We see that using PCA to decorrelate features is performing slightly worse than simple removal of correlated variables. 

Let us find the best tuning parameters with respect to accuray:
```{r}
titanic_knn_best_parm <-
   titanic_knn_results %>% 
   extract_workflow_set_result("NoDecor_kNN") %>% 
   select_best(metric="accuracy")
titanic_knn_best_parm
```
So the best option is kNN with n=10, Epanechnikov weight function and L2-distance.

We can now define best workflow and fit to the training data
```{r}
titanic_knn_best_wf <- titanic_knn_results %>% 
  extract_workflow("NoDecor_kNN") %>% 
  finalize_workflow(titanic_knn_best_parm)

titanic_knn_best_mod <- fit(titanic_knn_best_wf,titanic_train_model)

titanic_knn_pred <- predict(titanic_knn_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_knn_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_knn_pred.csv")
```

The Kaggle score, is 0.76315. The accuracy is exactly as for the logistic regression model.

Lets us collect the results in a Tibble, which we can use at the end of the chapter:
```{r}
results_table <- results_table %>% add_row(Model = "10 Nearest Neighbors, Epanechnikov weight, L2 distance", Score="0.76315")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

### SVM

The Support Vector Machine relies heavily on preprocessing of data. There are no built in room in the algorithm for dirty data. From the two previous sections it seems that PCA is less favourable for these data, so we will rely on simple decorrolation in this section. For normalizing we will use the BestNormalizer on the data. This leaves us with the following preprocessing recipe:

```{r}
titanic_svm_rec <- 
  titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_best_normalize(all_predictors())
```

In tidymodels the SVM kernel is defined from the model. We will define three models using respectively Linear, Polynomial and Radial Basis Function kernels.

```{r}
titanic_svm_lin <- svm_linear(cost=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab", scaled=TRUE)
titanic_svm_pol <- svm_poly(cost=tune(),degree=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab", scaled=TRUE)
titanic_svm_radial <- svm_rbf(cost=tune(),rbf_sigma=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab", scaled=TRUE)
```

Now we can define the workflow set for the different models and recipes:
```{r}
titanic_svm_wfs <- 
  workflow_set(
    preproc = list(rec = titanic_svm_rec),
    models = list(Lin = titanic_svm_lin,
                  Poly = titanic_svm_pol,
                  RBF = titanic_svm_radial)
  )
```

With different number of tuning parameters in the three models, we will rely on tidymodels to choose and apropriate grid of size 25 for the parameters.

We are now ready to fit at tune the various models:
```{r, warning=FALSE}
titanic_svm_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_svm_results <- 
  titanic_svm_wfs %>% 
  workflow_map(
    seed = 1234,
    resamples = titanic_folds,
    grid = 25,
    control = titanic_svm_control, 
    metrics = metric_set(accuracy,roc_auc),
    verbose = FALSE
  )
```
Let us have a look at the results:
```{r}
titanic_svm_results %>% 
  rank_results(select_best=TRUE) %>% 
  filter(.metric == "accuracy")
```
We see that the best performing model uses a Radial Basis Function kernel.

Let us find the best tuning parameters with respect to accuracy:
```{r}
titanic_svm_best_parm <-
   titanic_svm_results %>% 
   extract_workflow_set_result("rec_RBF") %>% 
   select_best(metric="accuracy")
titanic_svm_best_parm
```
So the best option is SVM with Radial Basis Function kernel, Cost parameter 6.35 and $\sigma = 0.186$.

We can now define best workflow and fit to the training data
```{r}
titanic_svm_best_wf <- titanic_svm_results %>% 
  extract_workflow("rec_RBF") %>% 
  finalize_workflow(titanic_svm_best_parm)

titanic_svm_best_mod <- fit(titanic_svm_best_wf,titanic_train_model)

titanic_svm_pred <- predict(titanic_svm_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_svm_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_svm_pred.csv")
```

The Kaggle score is 0.77272, which is a slight improvement over the previous models, but still inferior when it comes to Kaggle.

Lets us add the results to our table
```{r}
results_table <- results_table %>% add_row(Model = "Support Vector Machine with Radial Basis Function Kernel", Score="0.77272")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

### Classification trees: C5.0

For the rest of this chapter we will turn our attention to tree based models. The first model is a simple classification tree based on the C5.0 algorithm.

In general classification trees are highly flexible, deal with NAs quite easily and thus require very little preprocessing. To test this we will look at a few different light preprocessing recipes. One is the basic recipe. One adds  decorrolation and one further imputes NAs:

```{r}
titanic_c50_rec <- 
  titanic_baserec

titanic_c50_impute_rec <-
  titanic_c50_rec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors())
```

For the model, we will use a decision tree with engine C5.0:
```{r}
titanic_c50_mod <-
  decision_tree(min_n=tune()) %>% 
    set_engine("C5.0") %>% 
    set_mode("classification")
```

Now we can define the workflow set for the different recipes:
```{r}
titanic_c50_wfs <- 
  workflow_set(
    preproc = list(rec = titanic_c50_rec,
                   impute = titanic_c50_impute_rec),
    models = list(c50 = titanic_c50_mod)
  )
```

With just a single tunable parameter, we rely on tidymodels to choose an appropriate grid of size 5 for the parameters.

We are now ready to fit at tune the various models:
```{r, warning=FALSE}
titanic_c50_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_c50_results <- 
  titanic_c50_wfs %>% 
  workflow_map(
    seed = 1234,
    resamples = titanic_folds,
    grid = 5,
    control = titanic_c50_control, 
    metrics = metric_set(accuracy,roc_auc),
    verbose = FALSE
  )
```
Let us have a plot of the fitted models and workflows:
```{r}
autoplot(titanic_c50_results,rank_metric="accuracy",select_best = TRUE) +
  geom_text(aes(y=0.45,label= wflow_id), angle=90) +
  lims(y=c(0.2,0.9)) +
  theme(legend.position = "none")
```
Quite interestingly we see almost similar accuracy from the two workflows, but ROC is higher for the recipe without imputation. Thus, we will continue with this workflow.

Let us find the best tuning parameters with respect to accuracy:
```{r}
titanic_c50_best_parm <-
   titanic_c50_results %>% 
   extract_workflow_set_result("rec_c50") %>% 
   select_best(metric="roc_auc")
titanic_c50_best_parm
```
So, the best tree has minimal node size 6.

We can now define best workflow and fit to the training data
```{r}
titanic_c50_best_wf <- titanic_c50_results %>% 
  extract_workflow("rec_c50") %>% 
  finalize_workflow(titanic_c50_best_parm)

titanic_c50_best_mod <- fit(titanic_c50_best_wf,titanic_train_model)

titanic_c50_pred <- predict(titanic_c50_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_c50_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_c50_pred.csv")
```

The Kaggle score is 0.78468, which is quite interesting, since simple classification trees are usually weak learnes. This will make the next two sections, where we look at bagging and boosting trees quite interesting.

Lets us add the results to our table
```{r}
results_table <- results_table %>% add_row(Model = "C5.0 classification tree with minimum leaf size 6", Score="0.78468")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

### Bagged tree models: Bagged C5.0 and Random Forests

After our initial tour de simple models, we will now turn our attention to ensemble learning methods. In this subsection we will investigate bagging methods for trees. Specifically we will have a look at traditional bagged C5.0 trees and Random Forests. Traditionally bagged trees have the disadvantage of generating a lot of correlated trees, which dilutes the advantages of ensembles a bit. By using random forests, this should be offset.

As for the simple classification C5.0 tree, bagged trees require very little preprocessing. To some extend, imputing missing values could lead to a difference in prediction. The Random Forest algorithm needs to have missing values imputed. Thus we will use a base recipe as well as the base recipe with imputation of missed values for preprocessing:
```{r}
titanic_bagged_rec <- 
  titanic_baserec

titanic_bagged_impute_rec <-
  titanic_bagged_rec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors())
```

For the model, we will use a decision tree with engine C5.0 or a random forest. Random forests will usually not overfit given a high number of trees. Thus we will not tune the number of trees:
```{r}
titanic_bagged_c50_mod <-
  bag_tree(min_n=tune()) %>% 
    set_engine("C5.0") %>% 
    set_mode("classification")

titanic_rf_mod <-
  rand_forest(mtry = tune(),trees = 1000, min_n=tune()) %>% 
    set_engine("ranger") %>% 
    set_mode("classification")
```

Now we can define the workflow set for the different recipes:
```{r}
titanic_bag_wfs <- 
  workflow_set(
    preproc = list(
                    # rec = titanic_bagged_rec,
                   impute = titanic_bagged_impute_rec),
    models = list(bag_c50 = titanic_bagged_c50_mod)
  )

titanic_rf_wfs <- 
  workflow_set(
    preproc = list(impute =  titanic_bagged_impute_rec),
    models = list(rf= titanic_rf_mod)
  )

titanic_bagged_wfs <- 
  bind_rows(titanic_bag_wfs,titanic_rf_wfs)
```
This time agian, we rely on tidymodels to choose an appropriate grid for the parameters.

We are now ready to fit and tune the various models:
```{r, warning=FALSE}
titanic_bagged_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_bagged_results <- 
  titanic_bagged_wfs %>% 
  workflow_map(
    seed = 1234,
    fn = "tune_bayes",
    resamples = titanic_folds,
    grid = 16,
    control = titanic_bagged_control, 
    metrics = metric_set(accuracy,roc_auc),
    verbose = TRUE
  )
```
Let us have a plot of the fitted models and workflows:
```{r}
autoplot(titanic_bagged_results,rank_metric="accuracy",select_best = TRUE) +
  geom_text(aes(y=0.65,label= wflow_id), angle=90) +
  lims(y=c(0.4,0.9)) +
  theme(legend.position = "none")
```
We see that the Random Forest performs slightly better, when it comes to accuracy, than the bagged C5.0, though ROC seems almost similar. Moreover, when can see that the bagged C5.0 benefits slightly from imputation of missing values.

Let us find the best tuning parameters with respect to accuracy:
```{r}
titanic_bagged_best_parm <-
   titanic_bagged_results %>% 
   extract_workflow_set_result("impute_rf") %>% 
   select_best(metric="accuracy")
titanic_bagged_best_parm
```
We see that the best Random Forest workflow uses 3 randomly sampled predictors in each tree and minimum 2 data points in each splitting note.

We can now define best workflow and fit to the training data
```{r}
titanic_bagged_best_wf <- titanic_bagged_results %>% 
  extract_workflow("impute_rf") %>% 
  finalize_workflow(titanic_bagged_best_parm)

titanic_bagged_best_mod <- fit(titanic_bagged_best_wf,titanic_train_model)

titanic_bagged_pred <- predict(titanic_bagged_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_bagged_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_bagged_pred.csv")
```

The Kaggle score is 0.76794, which remarkably is worse than for the single C5.0 tree. Lets try submitting the best bagged C5.0 results as well:
```{r}
titanic_bagged_c50_best_parm <-
   titanic_bagged_results %>% 
   extract_workflow_set_result("impute_bag_c50") %>% 
   select_best(metric="roc_auc")
titanic_bagged_c50_best_parm

titanic_bagged_c50_best_wf <- titanic_bagged_results %>% 
  extract_workflow("impute_bag_c50") %>% 
  finalize_workflow(titanic_bagged_c50_best_parm)

titanic_bagged_c50_best_mod <- fit(titanic_bagged_c50_best_wf,titanic_train_model)

titanic_bagged_c50_pred <- predict(titanic_bagged_c50_best_mod,new_data=titanic_test)

cbind(PassengerId = titanic_test$PassengerId,titanic_bagged_c50_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_bagged_c50_pred.csv")
```
The Kaggle score for the bagged C5.0 is 0.78708, and thus it outperforms the random forest. One might ask if the random forest actually is overfit, even though they should have a tendency to do so.

Lets us add the results to our table
```{r}
results_table <- results_table %>% add_row(Model = "Random Forest with mtry=3 and min_n = 2", Score="0.76794") %>% add_row(Model = "Bagged C5.0 minimum leaf size 7", Score="0.78708")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

### Boosted tree models: XGBoost and lightGBM.

In this final subsection of models we will have a look at a different class of ensemble methods. Where bagged trees rely on bootstrapping the data to build a host of trees, boosting relies on building trees, that capture the features that have not yet been explained by the previous models. Thus, boosting is an example of a slow learner. And just as in the real world, slow learning often proves to be the most efficient. We will here have a look at two algorithms. The first is XGBoost, well known for being one of the most efficient and successful classification algorithms. The downside is, that the algorithm has a wide range of tunable parameters, and thus it can be costly to fit to data. Thus, we will also take a look at lightGBM, which can be up to 7 times faster to fit and tune than XGBoost, but has a small risk of overfitting.

Reading the documentation for the engines, XGBoost requires factors to be dummy encoded and missing values to be imputed. This gives us the following recipe for the boosting models:
```{r}
titanic_boost_rec <- titanic_baserec %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors())
```

Now we can define the models:
```{r}
titanic_xgb_mod <- boost_tree(trees=tune(),min_n=tune(),tree_depth=tune(),
                              learn_rate = tune(), loss_reduction = tune()) %>% 
                     set_mode("classification") %>% 
                     set_engine("xgboost")

titanic_lgb_mod <- boost_tree(trees=tune(),min_n=tune(),tree_depth=tune(),
                              learn_rate = tune(), loss_reduction = tune()) %>% 
                     set_mode("classification") %>% 
                     set_engine("lightgbm")
```

The associated grid is
```{r}
titanic_boost_grid <- grid_regular(trees(),min_n(),tree_depth(),
                                   learn_rate(),loss_reduction(),levels=5) 
```

Now we can define the workflow sets:
```{r}
titanic_boost_wfs <- 
  workflow_set(
    preproc = list(rec =  titanic_boost_rec),
    models = list(xgb= titanic_xgb_mod)
  )
```

With a grid of size $5^4=625$ and relatively expensive models to fit, we will use tune_racing_anova to speed up the process. Tune_racing_anova is a method where is set of parameters is initially screened and obviously inefficient sets are discarded. Another option could have been to use a spacefilling grid instead of a regular grid.

```{r, warning=FALSE}
titanic_boost_control <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

titanic_boost_results <- 
  titanic_boost_wfs %>% 
  workflow_map(
    seed = 1234,
    resamples = titanic_folds,
    grid = titanic_boost_grid,
    control = titanic_boost_control,
    verbose = TRUE,
    objective = "binary:logistic",
    eval_metric = "logloss"
  )
```
Let us have a plot of the fitted models and workflows:
```{r}
autoplot(titanic_boost_results,rank_metric="accuracy",select_best = TRUE) +
  geom_text(aes(y=0.65,label= wflow_id), angle=90) +
  lims(y=c(0.4,0.9)) +
  theme(legend.position = "none")
```
We see that both accuracy and ROC are in the same ballpark as for the other models. It will be interesting to see if the XGBoost is better at generalizing the results to the test data.

Let us find the best tuning parameters with respect to accuracy:
```{r}
titanic_boost_best_parm <-
   titanic_boost_results %>% 
   extract_workflow_set_result("rec_xgb") %>% 
   select_best(metric="accuracy")
titanic_boost_best_parm
```
We see that the best results are obtained with 500 trees, minimum node size when splitting of 11, a tree depth of each tree of 4 and a learning rate of 0.1. The learning rate is seen to be lower than the default learning rate for the XGBoost engine of 0.3

We can now define best workflow and fit to the training data
```{r}
titanic_boost_best_wf <- titanic_boost_results %>% 
  extract_workflow("rec_xgb") %>% 
  finalize_workflow(titanic_boost_best_parm)

titanic_boost_best_mod <- fit(titanic_boost_best_wf,titanic_train_model)

titanic_boost_pred <- predict(titanic_boost_best_mod,new_data=titanic_test)
```
We are now ready to write the predictions to a file and upload to Kaggle:
```{r}
cbind(PassengerId = titanic_test$PassengerId,titanic_boost_pred) %>% 
  rename(Survived = .pred_class) %>% 
  write_csv("./titanic_boost_pred.csv")
```

The Kaggle score is 0., which

Lets us add the results to our table
```{r}
results_table <- results_table %>% add_row(Model = "XGBoost with", Score="0.76794")
results_table %>% kbl() %>% kable_material(c("striped","hover"))
```

## Summary

## Shiny classification app


