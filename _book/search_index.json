[["titanic---studies-on-classification.html", "Chapter 3 Titanic - studies on classification 3.1 Data collection and cleaning 3.2 EDA 3.3 Modelling", " Chapter 3 Titanic - studies on classification In this chapter we will turn our focus to classification techniques. The data set used will be the Kaggle data set Titanic. The first section will focus on data collection and data wrangling. The second section has its focus on EDA. In the third and last section we will study the following classification models and how to optimize them for the dataset: Logistic Regression, LDA, kNN, SVM, XGBoost and lightGBM. The evaluation metric will be the Kaggle score for each model. 3.1 Data collection and cleaning The data was downloaded from (Kaggle)[https://www.kaggle.com/c/titanic]. The data set is made up of two parts. Part 1 is a training set for supervised learning. Part 2 is a test set, where the outcome column has been removed. We will begin by loading the two data files and have a look at the data types of each column: titanic_train &lt;- read_csv(&quot;./titanic_train.csv&quot;) ## Rows: 891 Columns: 12 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Name, Sex, Ticket, Cabin, Embarked ## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. titanic_test &lt;- read_csv(&quot;./titanic_test.csv&quot;) ## Rows: 418 Columns: 11 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Name, Sex, Ticket, Cabin, Embarked ## dbl (6): PassengerId, Pclass, Age, SibSp, Parch, Fare ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We see that the training data is comprised of 891 observation of 12 features, whereas the test data is comprised of 418 observations of 11 features. We have 5 features coded as charaters: Name, Sex, Ticket, Cabin, Embarked. We have 6 numeric features: PassengerId, Pclass, Age, SibSp. Parch, Fare. Let us first see if we need to deal with missing values: titanic_train %&gt;% select(which(colSums(is.na(.))&gt;0)) %&gt;% summarise_all(~ sum(is.na(.))) ## # A tibble: 1 × 3 ## Age Cabin Embarked ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 177 687 2 We see that most of the cabin data is missing. We will not deal with these for now, but drop the column, when doing the actual modelling later. For embarked we will replace the missing values with the mode. We start by determining the mode: titanic_train %&gt;% group_by(Embarked) %&gt;% count() %&gt;% arrange(desc(n)) ## # A tibble: 4 × 2 ## # Groups: Embarked [4] ## Embarked n ## &lt;chr&gt; &lt;int&gt; ## 1 S 644 ## 2 C 168 ## 3 Q 77 ## 4 &lt;NA&gt; 2 Then we replace the NAs by “S”: titanic_train &lt;- titanic_train %&gt;% mutate(Embarked = replace(Embarked,is.na(Embarked),&quot;S&quot;)) Next, let’s have a look at the distribution of age: titanic_train %&gt;% count(Age_interval = cut_interval(Age,n=6)) ## # A tibble: 7 × 2 ## Age_interval n ## &lt;fct&gt; &lt;int&gt; ## 1 [0.42,13.7] 71 ## 2 (13.7,26.9] 248 ## 3 (26.9,40.2] 245 ## 4 (40.2,53.5] 100 ## 5 (53.5,66.7] 43 ## 6 (66.7,80] 7 ## 7 &lt;NA&gt; 177 We get see that Age seems right skewed. To verify this we calculate the skewness using the {moments} package: titanic_train %&gt;% select(Age) %&gt;% filter(!is.na(.)) %&gt;% summarise(Skewness = moments::skewness(.)) ## # A tibble: 1 × 1 ## Skewness ## &lt;dbl&gt; ## 1 0.388 With the right skewed age data we choose to impute the missing values in Age column with the median. titanic_train &lt;- titanic_train %&gt;% mutate(Age = replace(Age,is.na(Age),median(Age,na.rm=TRUE))) With this we conclude the data collection and preparation process and move on to the EDA. 3.2 EDA Lets begin by looking at the prior distrubution of survivals: titanic_train %&gt;% mutate(Survived = as_factor(Survived)) %&gt;% mutate(Survived = fct_recode(Survived, &quot;Died&quot;=&quot;0&quot;,&quot;Survived&quot;=&quot;1&quot;)) %&gt;% ggplot(aes(x=Survived,fill=Survived)) + geom_bar(show.legend = FALSE) + labs(x=&quot;Survival Status&quot;, y=&quot;Passenger Count&quot;, title = &quot;Distribution of passenger status&quot;) We see that roughly 3 of 5 died in the Titanic disaster. We now turn our attention to finding reasonable correlations in the data. A place to start would be a correlation diagram. We can choose to look only at the numerical features in the data set: titanic_train %&gt;% select(which(sapply(.,is.numeric))) %&gt;% correlate(diagonal = 1,quiet=TRUE) %&gt;% rplot() ## Don&#39;t know how to automatically pick scale for object of type noquote. Defaulting to continuous. Alternatively we can one hot encode the relevant categorical variables and include them as well: recipe(Survived ~ .,data=titanic_train) %&gt;% update_role(PassengerId,Name,Ticket,Cabin,new_role=&quot;ID&quot;) %&gt;% step_dummy(all_nominal_predictors(),one_hot=TRUE) %&gt;% prep(retain=TRUE) %&gt;% juice() %&gt;% select(which(sapply(.,is.numeric))) %&gt;% correlate(diagonal = 1) %&gt;% rplot() + theme(legend.position = &quot;top&quot;) ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; ## Don&#39;t know how to automatically pick scale for object of type noquote. Defaulting to continuous. From the correlation plot we see that Fare and PClass are moderately correlated. This can become an issue later on in the modelling process. We also see that Survived primarily is correlated to Sex and PClass. Lets investigate these further. First let us have a look at how survived is related to sex: titanic_train %&gt;% mutate(Survived = as_factor(Survived)) %&gt;% mutate(Survived = fct_recode(Survived, &quot;Died&quot;=&quot;0&quot;,&quot;Survived&quot;=&quot;1&quot;)) %&gt;% ggplot(aes(x=Sex, fill=Survived)) + geom_bar(position=&quot;dodge&quot;) As expected the two variables are correlated. A higher proportion of women survived compared to men. Next let us have a look at the Pclass vs Survived: titanic_train %&gt;% mutate(Survived = as_factor(Survived)) %&gt;% mutate(Pclass = as_factor(Pclass)) %&gt;% mutate(Survived = fct_recode(Survived, &quot;Died&quot;=&quot;0&quot;,&quot;Survived&quot;=&quot;1&quot;)) %&gt;% ggplot(aes(x=Pclass, fill=Survived)) + geom_bar(position=&quot;dodge&quot;) From this we see that a 1st class passenger was more likely to survive than a 3rd class passenger. Finally we can have a look at the fare distributions of the passengers: titanic_train %&gt;% mutate(Survived = as_factor(Survived)) %&gt;% mutate(Survived = fct_recode(Survived, &quot;Died&quot;=&quot;0&quot;,&quot;Survived&quot;=&quot;1&quot;)) %&gt;% ggplot(aes(x=Fare,color=Survived)) + stat_density(geom=&quot;line&quot;) We see that among those that died, there is a higher proportion of passengers with a low fare. This is easily explained from the higher proportion of 3rd class passengers among those who died. 3.3 Modelling In this section we will develop several models to predict if a passenger will survive or not. Different model classes have different requirements to feature engineering, so we will develop several preprocessing recipes and choose the ones that performs best with respect to accuracy and ROC. Since some models are able to deal well with NAs we reload the original dataset with missing values for modelling purposes: titanic_train_model &lt;- read_csv(&quot;./titanic_train.csv&quot;) ## Rows: 891 Columns: 12 ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Name, Sex, Ticket, Cabin, Embarked ## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. For the classification models we need the outcome to be a factor variable: titanic_train_model &lt;- titanic_train_model %&gt;% mutate(Survived = as_factor(Survived)) Let us have a look at the skewness of the different numerical variables to decide upon which kind of imputation to choose: titanic_train_model %&gt;% select(which(sapply(.,is.numeric)),-PassengerId) %&gt;% summarise_all(~ moments::skewness(.)) ## # A tibble: 1 × 5 ## Pclass Age SibSp Parch Fare ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.629 NA 3.69 2.74 4.78 We see that all variables are skewed, so we impute them using the median. All models use the same formula and need to have the same columns as ID columns, so we begin by making a base recipe for the preprocessing: titanic_baserec &lt;- recipe(Survived ~ ., data=titanic_train_model) %&gt;% step_rm(Ticket,Cabin,Name) %&gt;% update_role(PassengerId,new_role=&quot;ID&quot;) To tune the different models we will use 10-fold cross validation. For this purpose we define the folds: titanic_folds &lt;- vfold_cv(titanic_train_model,strata = Survived) 3.3.1 Logistic Regression Logistic regression relies on the features being independent. Moreover it doesn’t do well with missing values and factors with small frequencies. This gives us the following recipe for logistic regression: titanic_logit_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) For decorrelation we can choose between PCA and removing correlated variables. Additionally we can decide to normalize the numeric predictors or not. If we do, we use Yeo-Johnson transformation. This gives us the following final recipes for logistic regression: titanic_logit_pca_nonorm &lt;- titanic_logit_rec %&gt;% step_pca(all_predictors()) titanic_logit_decor_nonorm &lt;- titanic_logit_rec %&gt;% step_corr(all_predictors()) titanic_logit_pca_norm &lt;- titanic_logit_rec %&gt;% step_pca(all_predictors()) %&gt;% step_YeoJohnson(all_numeric_predictors()) titanic_logit_decor_norm &lt;- titanic_logit_rec %&gt;% step_corr(all_predictors()) %&gt;% step_YeoJohnson(all_numeric_predictors()) For models we can use a regularized or un-regularized logistic regression model. For the regularized model we will using 10-fold cross validation tuning to choose the best regularization parameters. titanic_logit_mod &lt;- logistic_reg() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;glm&quot;) titanic_reg_logit_mod &lt;- logistic_reg(penalty=tune(),mixture=tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;glmnet&quot;) Now we can define the workflow set for the different models and recipes: titanic_logit_wfs &lt;- workflow_set( preproc = list(DeCorNoNorm = titanic_logit_decor_nonorm, DeCorNorm = titanic_logit_decor_norm, PCANoNorm = titanic_logit_pca_nonorm, PCANorm = titanic_logit_pca_norm), models = list(regularized = titanic_reg_logit_mod, unregularized =titanic_logit_mod) ) For tuning the regularized model we define the following tuning grid: titanic_logit_grid &lt;- grid_regular(penalty(),mixture(),levels=c(5,2)) We are now ready to fit at tune the various models: titanic_logit_control &lt;- control_grid( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE ) titanic_logit_results &lt;- titanic_logit_wfs %&gt;% workflow_map( seed = 1234, resamples = titanic_folds, grid = titanic_logit_grid, control = titanic_logit_control, metrics = metric_set(accuracy,roc_auc), verbose = FALSE ) Let us have a plot of the fitted models and workflows: autoplot(titanic_logit_results,rank_metric=&quot;accuracy&quot;,select_best = TRUE) We see that half of the combinations perform significantly better than the others. Lets have a look at the results: titanic_logit_results %&gt;% rank_results(select_best = TRUE) %&gt;% filter(.metric == &quot;accuracy&quot;) ## # A tibble: 8 × 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 DeCorNoNorm_regularized Preprocessor1… accura… 0.798 0.0127 10 recipe logi… 1 ## 2 DeCorNoNorm_unregularized Preprocessor1… accura… 0.793 0.0118 10 recipe logi… 2 ## 3 DeCorNorm_regularized Preprocessor1… accura… 0.792 0.0127 10 recipe logi… 3 ## 4 DeCorNorm_unregularized Preprocessor1… accura… 0.792 0.0115 10 recipe logi… 4 ## 5 PCANoNorm_unregularized Preprocessor1… accura… 0.734 0.0141 10 recipe logi… 5 ## 6 PCANoNorm_regularized Preprocessor1… accura… 0.733 0.0142 10 recipe logi… 6 ## 7 PCANorm_regularized Preprocessor1… accura… 0.725 0.0134 10 recipe logi… 7 ## 8 PCANorm_unregularized Preprocessor1… accura… 0.722 0.0146 10 recipe logi… 8 We see that using PCA to decorrelate features is performing worse than simple removal of correlated variables. Let us find the best tuning parameters: titanic_logit_best_parm &lt;- titanic_logit_results %&gt;% extract_workflow_set_result(&quot;DeCorNoNorm_regularized&quot;) %&gt;% select_best(metric=&quot;accuracy&quot;) titanic_logit_best_parm ## # A tibble: 1 × 3 ## penalty mixture .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00316 1 Preprocessor1_Model09 So the best option is Lasso regression with a slight penalty of 1.0e-10. titanic_logit_best_wf &lt;- titanic_logit_results %&gt;% extract_workflow(&quot;DeCorNoNorm_regularized&quot;) %&gt;% finalize_workflow(titanic_logit_best_parm) titanic_logit_best_mod &lt;- fit(titanic_logit_best_wf,titanic_train_model) titanic_logit_pred &lt;- predict(titanic_logit_best_mod,new_data=titanic_test) We are now ready to write the predictions to a file and upload to Kaggle: cbind(PassengerId = titanic_test$PassengerId,titanic_logit_pred) %&gt;% rename(Survived = .pred_class) %&gt;% write_csv(&quot;./titanic_logit_pred.csv&quot;) The Kaggle score (which established the baseline for this chapter), is 0.76315. The accuracy is as expected slightly lower than for the training data, but a very respectable result for a linear model. Lets us collect the results in a Tibble, which we can use at the end of the chapter: results_table &lt;- tibble(Model = c(&quot;L2-penalized Logistic Regression&quot;), Score = &quot;0.76315&quot;) results_table %&gt;% kbl() %&gt;% kable_material(c(&quot;striped&quot;,&quot;hover&quot;)) Model Score L2-penalized Logistic Regression 0.76315 3.3.2 LDA The linear discriminant classifier (LDA-C) is mostly included for reference. In general it is not expected to perform as well as the penalized logistic regression. Pre-processing for the LDA is almost the same as for logistic regression. We need to remove zero variance variables as well as ID variables. Moreover we need to one-hot encode factors and and de-corrolate features. Finally LDA might benefit from normalization of variables, in which case we use Yeo Johnson transformation. This leaves us with the following preprocessing recipes: titanic_lda_nonorm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) titanic_lda_norm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% step_YeoJohnson(all_numeric_predictors()) titanic_lda_decor_nonorm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% step_corr(all_predictors()) titanic_lda_decor_norm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% step_YeoJohnson(all_numeric_predictors()) %&gt;% step_corr(all_predictors()) titanic_lda_pca_nonorm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% step_pca(all_predictors()) titanic_lda_pca_norm_rec &lt;- titanic_baserec %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_impute_mode(all_nominal_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% step_YeoJohnson(all_numeric_predictors()) %&gt;% step_pca(all_predictors()) This section is temporarily aborted due to problems with {Discrim} package. 3.3.3 kNN 3.3.4 SVM 3.3.5 XGBoost 3.3.6 lightGBM. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
