[["index.html", "Data Science and Tidymodels Case Studies Chapter 1 Introduction", " Data Science and Tidymodels Case Studies Bjarne Lund Pedersen 2022-02-17 Chapter 1 Introduction In this book I will give examples of the use of Tidyverse packages in data science studies. Chapter 1 contains a transcription of the final capstone project from the IBM Python Data Science professional certificate. Emphasis in this chapter will primarily be on data collection, data cleaning, data wrangling and EDA, while modelling will play a minor role. Chapter 2 contains various modelling techniques for classificiation including logit-regression, classification trees, LDA, SVM and boosting trees using XGBoost. The Kaggle Titinic dataset will be used for this purpose. Chapter 3 contaings various modelling techniques for regression including linear regression, generalized linear models, generalized additive models and boosting trees for regression. The Kaggle Ames housing dataset will be used in this chapter. Chapter 4 contains modelling techniques for time-series using the Kaggle Store Sales dataset. Chapter 5 contains image recognition using Neural Networks. Fror this purpose we will use the Kaggel “Petal to the Metal” dataset. "],["machine-learning-capstone-project.html", "Chapter 2 Machine Learning Capstone Project 2.1 Data Collection 2.2 Data Wrangling 2.3 EDA with Visualizations 2.4 Interactive Visual Analytics 2.5 Machine Learning Models", " Chapter 2 Machine Learning Capstone Project In this chapter we will have a look at the SpaceX dataset, which is used for the final capstone project in IBM Python Data Science Professional Certificate. Originally the project is Python based, but my aim is to do the exact things in R using the Tidyverse dialect. The project contains the following parts: Data , Data Wrangling, Data Visualizations, Interactive Maps using Leaflet, R Shiny app and Machine Learning models. 2.1 Data Collection In this chapter we will collect the data for the SpaceX project using an API. First we will connect to an URL to retrieve the data: spacex_url=&#39;https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/API_call_spacex_api.json&#39; response = httr::GET(spacex_url) print(response$status_code) ## [1] 200 print(response) ## Response [https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/API_call_spacex_api.json] ## Date: 2022-02-17 12:46 ## Status: 200 ## Content-Type: application/json ## Size: 270 kB With status code 200 the connection was successful. Let us retrieve the JSON content using the jsonlite::fromJSON: data = fromJSON(spacex_url) We notice that a lot of the data are IDs. For example the rocket column has no information about the rocket just an identification number. We will now use the API again to get information about the launches using the IDs given for each launch. Specifically we will be using columns rocket, paayloads, launchpad, and cores. Moreover we need to extract primary payload, extract the date of date_utc as well as unfold the cores tibble into separate columns. data &lt;- data %&gt;% select(rocket,payloads,launchpad,cores,flight_number,date_utc) %&gt;% hoist(payloads,payload_new = 1) %&gt;% unnest(cores) %&gt;% select(-c(&quot;payloads&quot;)) %&gt;% rename(payloads = payload_new) %&gt;% mutate(Date = date(date_utc)) Let us have a look at the data types of the columns: str(data) ## tibble [113 × 15] (S3: tbl_df/tbl/data.frame) ## $ rocket : chr [1:113] &quot;5e9d0d95eda69955f709d1eb&quot; &quot;5e9d0d95eda69955f709d1eb&quot; &quot;5e9d0d95eda69955f709d1eb&quot; &quot;5e9d0d95eda69955f709d1eb&quot; ... ## $ payloads : chr [1:113] &quot;5eb0e4b5b6c3bb0006eeb1e1&quot; &quot;5eb0e4b6b6c3bb0006eeb1e2&quot; &quot;5eb0e4b6b6c3bb0006eeb1e3&quot; &quot;5eb0e4b7b6c3bb0006eeb1e5&quot; ... ## $ launchpad : chr [1:113] &quot;5e9e4502f5090995de566f86&quot; &quot;5e9e4502f5090995de566f86&quot; &quot;5e9e4502f5090995de566f86&quot; &quot;5e9e4502f5090995de566f86&quot; ... ## $ core : chr [1:113] &quot;5e9e289df35918033d3b2623&quot; &quot;5e9e289ef35918416a3b2624&quot; &quot;5e9e289ef3591814873b2625&quot; &quot;5e9e289ef3591855dc3b2626&quot; ... ## $ flight : int [1:113] 1 1 1 1 1 1 1 1 1 1 ... ## $ gridfins : logi [1:113] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ legs : logi [1:113] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ reused : logi [1:113] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ landing_attempt: logi [1:113] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ landing_success: logi [1:113] NA NA NA NA NA NA ... ## $ landing_type : chr [1:113] NA NA NA NA ... ## $ landpad : chr [1:113] NA NA NA NA ... ## $ flight_number : int [1:113] 1 2 3 4 5 6 7 8 9 10 ... ## $ date_utc : chr [1:113] &quot;2006-03-24T22:30:00.000Z&quot; &quot;2007-03-21T01:10:00.000Z&quot; &quot;2008-08-03T03:34:00.000Z&quot; &quot;2008-09-28T23:15:00.000Z&quot; ... ## $ Date : Date[1:113], format: &quot;2006-03-24&quot; &quot;2007-03-21&quot; &quot;2008-08-03&quot; &quot;2008-09-28&quot; ... We will now use the API again to get information about the launches using the IDs given for each launch. Specifically we will be using columns rocket, payloads, launchpad, and cores. From the rocket we would like to learn the booster name From the payload we would like to learn the mass of the payload and the orbit that it is going to From the launchpad we would like to know the name of the launch site being used, the longitude, and the latitude. From cores we would like to learn the outcome of the landing, the type of the landing, number of flights with that core, whether gridfins were used, whether the core is reused, whether legs were used, the landing pad used, the block of the core which is a number used to seperate version of cores, the number of times this specific core has been reused, and the serial of the core. data &lt;- data %&gt;% add_column(BoosterVersion=NA,Longitude=NA,Latitude=NA,LaunchSite=NA,PayloadMass=NA,Orbit=NA,ReusedCount=NA,Block=NA, Serial=NA) for (i in 1:nrow(data)){ rocket_url = str_c(&quot;https://api.spacexdata.com/v4/rockets/&quot;,data$rocket[i]) launchpad_url = str_c(&quot;https://api.spacexdata.com/v4/launchpads/&quot;,data$launchpad[i]) payloads_url = str_c(&quot;https://api.spacexdata.com/v4/payloads/&quot;,data$payloads[i]) core_url = str_c(&quot;https://api.spacexdata.com/v4/cores/&quot;,data$core[i]) response_rocket = fromJSON(rocket_url) response_launchpad = fromJSON(launchpad_url) response_payloads = fromJSON(payloads_url) response_core = fromJSON(core_url) data$BoosterVersion[i] &lt;- ifelse(is.null(response_rocket$name),NA, response_rocket$name) data$Longitude[i] &lt;- ifelse(is.null(response_launchpad$longitude),NA, response_launchpad$longitude) data$Latitude[i] &lt;- ifelse(is.null(response_launchpad$latitude),NA, response_launchpad$latitude) data$LaunchSite[i] &lt;- ifelse(is.null(response_launchpad$name),NA, response_launchpad$name) data$PayloadMass[i] &lt;- ifelse(is.null(response_payloads$mass_kg),NA, response_payloads$mass_kg) data$Orbit[i] &lt;- ifelse(is.null(response_payloads$orbit),NA, response_payloads$orbit) data$ReusedCount[i] &lt;- ifelse(is.null(response_core$reuse_count),NA, response_core$reuse_count) data$Block[i] &lt;- ifelse(is.null(response_core$block),NA, response_core$block) data$Serial[i] &lt;- ifelse(is.null(response_core$serial),NA, response_core$serial) } The outcome of a launch will be a concenation of “landing_success” and “landing_type”: data$Outcome = str_c(data$landing_success,data$landing_type,sep=&quot; &quot;) Now we select the relevant columns and filter the observations where Falcon 9 was used. data &lt;- data %&gt;% select(FlightNumber,Date,BoosterVersion,PayloadMass,Orbit, LaunchSite,Outcome,Flights,GridFins,Reused, Legs,LandingPad,Block,ReusedCount,Serial,Longitude, Latitude) %&gt;% filter(BoosterVersion == &quot;Falcon 9&quot;) Let us have a final look at the data before we move on to data wrangling: head(data,width=Inf) 2.2 Data Wrangling In this section we will take a look at the data collected in the previous section. We will deal with missing values and introduce an outcome class variable to distinguish between successful and unsuccessful landings. In case the data collection fails, we begin by loading the pre-downloaded data set: Lets have a glimpse at the dataframe: glimpse(data) ## Rows: 99 ## Columns: 17 ## $ FlightNumber &lt;dbl&gt; 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48… ## $ Date &lt;date&gt; 2010-06-04, 2010-12-08, 2012-05-22, 2012-10-08, 2013-03-01, 2013-09-29, 2013-12-03, 2014-01-06, 2014-04-18, 2014-07-14, 2014-08-05, 2014-09-07, 2014-09-21, 2015-01-1… ## $ BoosterVersion &lt;chr&gt; &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;, &quot;Falcon 9&quot;… ## $ PayloadMass &lt;dbl&gt; NA, NA, 525.00, 400.00, 677.00, 500.00, 3170.00, 3325.00, 2296.00, 1316.00, 4535.00, 4428.00, 2216.00, 2395.00, 570.00, 1954.00, 1898.00, 4707.00, 2477.00, 2034.00, 5… ## $ Orbit &lt;chr&gt; &quot;LEO&quot;, &quot;LEO&quot;, &quot;LEO&quot;, &quot;ISS&quot;, &quot;ISS&quot;, &quot;PO&quot;, &quot;GTO&quot;, &quot;GTO&quot;, &quot;ISS&quot;, &quot;LEO&quot;, &quot;GTO&quot;, &quot;GTO&quot;, &quot;ISS&quot;, &quot;ISS&quot;, &quot;ES-L1&quot;, &quot;GTO&quot;, &quot;ISS&quot;, &quot;GTO&quot;, &quot;ISS&quot;, &quot;LEO&quot;, &quot;PO&quot;, &quot;GTO&quot;, &quot;ISS&quot;, &quot;GTO&quot;… ## $ LaunchSite &lt;chr&gt; &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;VAFB SLC 4E&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS SLC 40&quot;, &quot;CCSFS … ## $ Outcome &lt;chr&gt; NA, NA, NA, NA, NA, &quot;FALSE Ocean&quot;, NA, NA, &quot;TRUE Ocean&quot;, &quot;TRUE Ocean&quot;, NA, NA, &quot;FALSE Ocean&quot;, &quot;FALSE ASDS&quot;, &quot;TRUE Ocean&quot;, NA, &quot;FALSE ASDS&quot;, NA, NA, &quot;TRUE RTLS&quot;, &quot;FALS… ## $ Flights &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2… ## $ GridFins &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Reused &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ Legs &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… ## $ LandingPad &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;5e9e3032383ecb761634e7cb&quot;, NA, NA, &quot;5e9e3032383ecb761634e7cb&quot;, NA, &quot;5e9e3032383ecb6bb234e7ca&quot;, &quot;5e9e3032383ecb267… ## $ Block &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 4, 4, 4, 4, 5, 4… ## $ ReusedCount &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1… ## $ Serial &lt;chr&gt; &quot;B0003&quot;, &quot;B0004&quot;, &quot;B0005&quot;, &quot;B0006&quot;, &quot;B0007&quot;, &quot;B1003&quot;, &quot;B1004&quot;, &quot;B1005&quot;, &quot;B1006&quot;, &quot;B1007&quot;, &quot;B1008&quot;, &quot;B1011&quot;, &quot;B1010&quot;, &quot;B1012&quot;, &quot;B1013&quot;, &quot;B1014&quot;, &quot;B1015&quot;, &quot;B1016&quot;, &quot;B10… ## $ Longitude &lt;dbl&gt; -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, -120.61083, -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, -80.57737, … ## $ Latitude &lt;dbl&gt; 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 34.63209, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.56186, 28.561… Lets first identify and calculate the percentage of the missing values in each attribute data %&gt;% select(which(colSums(is.na(.))&gt;0)) %&gt;% summarise_all(~ sum(is.na(.))/n()*100) ## # A tibble: 1 × 3 ## PayloadMass Outcome LandingPad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.07 26.3 31.3 Missing values in Outcome refers to both Landing_success and Landing_type being false. These will be replaced by “FALSE FALSE.” We then see, that in the data set, there are several different cases where the booster did not land successfully. Sometimes a landing was attempted but failed due to an accident; for example, True Ocean means the mission outcome was successfully landed to a specific region of the ocean while False Ocean means the mission outcome was unsuccessfully landed to a specific region of the ocean. True RTLS means the mission outcome was successfully landed to a ground pad False RTLS means the mission outcome was unsuccessfully landed to a ground pad.True ASDS means the mission outcome was successfully landed on a drone ship False ASDS means the mission outcome was unsuccessfully landed on a drone ship. data &lt;- data %&gt;% mutate(Outcome = replace(Outcome,is.na(Outcome),&quot;FALSE FALSE&quot;) ) To deal with missing values in PayloadsMass we replace NA with mean payload mass: data &lt;- data %&gt;% mutate(PayloadMass = replace(PayloadMass,is.na(PayloadMass),mean(PayloadMass,na.rm=TRUE)) ) This leaves us with only the LandingPad column containing missing values. Since most models cannot handle NA, these will be replaced with FALSE: data &lt;- data %&gt;% mutate(LandingPad = replace_na(LandingPad,&quot;FALSE&quot;)) The data contains several Space X launch facilities: Cape Canaveral Space Launch Complex 40 VAFB SLC 4E , Vandenberg Air Force Base Space Launch Complex 4E (SLC-4E), Kennedy Space Center Launch Complex 39A KSC LC 39A .The location of each Launch Is placed in the column LaunchSite Next, let’s see the number of launches for each site. data %&gt;% group_by(LaunchSite) %&gt;% count() ## # A tibble: 3 × 2 ## # Groups: LaunchSite [3] ## LaunchSite n ## &lt;chr&gt; &lt;int&gt; ## 1 CCSFS SLC 40 60 ## 2 KSC LC 39A 24 ## 3 VAFB SLC 4E 15 Then let’s have a look at the different orbits used data %&gt;% group_by(Orbit) %&gt;% count() ## # A tibble: 11 × 2 ## # Groups: Orbit [11] ## Orbit n ## &lt;chr&gt; &lt;int&gt; ## 1 ES-L1 1 ## 2 GEO 1 ## 3 GTO 30 ## 4 HEO 1 ## 5 ISS 23 ## 6 LEO 8 ## 7 MEO 3 ## 8 PO 10 ## 9 SO 1 ## 10 SSO 6 ## 11 VLEO 15 Finally let us have a look at the different landing outcomes data %&gt;% group_by(Outcome) %&gt;% count() ## # A tibble: 7 × 2 ## # Groups: Outcome [7] ## Outcome n ## &lt;chr&gt; &lt;int&gt; ## 1 FALSE ASDS 7 ## 2 FALSE FALSE 26 ## 3 FALSE Ocean 2 ## 4 FALSE RTLS 1 ## 5 TRUE ASDS 44 ## 6 TRUE Ocean 5 ## 7 TRUE RTLS 14 True Ocean means the mission outcome was successfully landed to a specific region of the ocean while False Ocean means the mission outcome was unsuccessfully landed to a specific region of the ocean. True RTLS means the mission outcome was successfully landed to a ground pad False RTLS means the mission outcome was unsuccessfully landed to a ground pad.True ASDS means the mission outcome was successfully landed to a drone ship False ASDS means the mission outcome was unsuccessfully landed to a drone ship. None ASDS and None None these represent a failure to land. For modelling purposes later, we would like to have a landing outcome variable “Class,” where all successful landings are denoted by a “1” and all other by a “0.” successful_outcomes=c(&quot;TRUE ASDS&quot;, &quot;TRUE Ocean&quot;, &quot;TRUE RTLS&quot;) data &lt;-data %&gt;% mutate(Class = ifelse(Outcome %in% successful_outcomes,1,0)) Let us finally find the average succesrate: data %&gt;% summarise(Class = mean(Class)) ## # A tibble: 1 × 1 ## Class ## &lt;dbl&gt; ## 1 0.636 With this we conclude the data wrangling section. In the next section we take a look at different visualizations of the dataset. 2.3 EDA with Visualizations First, let’s try to see how the FlightNumber (indicating the continuous launch attempts.) and Payload variables would affect the launch outcome. We can plot out the FlightNumber vs. PayloadMassand overlay the outcome of the launch. We see that as the flight number increases, the first stage is more likely to land successfully. The payload mass is also important; it seems the more massive the payload, the less likely the first stage will return. data %&gt;% ggplot(mapping = aes(x=FlightNumber,y=PayloadMass,color=as.factor(Class))) + geom_point(size = 2) + scale_color_hue(&quot;Mission Outcome&quot;, breaks = c(0,1), labels = c(&quot;Failure&quot;,&quot;Success&quot;) ) + labs(x=&quot;Flight Number&quot;, y=&quot;Payload Mass (kg)&quot;, color=&quot;Outcome&quot;, title=&quot;Payload Mass vs. Flight Number&quot;) Next we will drill down to each launch site and look at their detailed launch records. data %&gt;% ggplot(mapping = aes(x=FlightNumber,y=LaunchSite,color=as.factor(Class))) + geom_point(size = 2) + scale_color_hue(&quot;Mission Outcome&quot;, breaks = c(0,1), labels = c(&quot;Failure&quot;,&quot;Success&quot;) ) + labs(x=&quot;Flight Number&quot;, y=&quot;Launch Site&quot;, color=&quot;Outcome&quot;, title=&quot;Launch Site vs. Flight Number&quot;) We see that the flight is more likely to succeed from VAFB SLC 4E and KSC LC 39A. Moreover VAFB SLC 4E has not been used for later launches. We also want to observe if there is any relationship between launch sites and their payload mass. data %&gt;% ggplot(mapping = aes(x=PayloadMass,y=LaunchSite,color=as.factor(Class))) + geom_point(size = 2) + scale_color_hue(&quot;Mission Outcome&quot;, breaks = c(0,1), labels = c(&quot;Failure&quot;,&quot;Success&quot;) ) + labs(x=&quot;Payload Mass (kg)&quot;, y=&quot;Launch Site&quot;, color=&quot;Outcome&quot;, title=&quot;Launch Site vs. Payload Mass&quot;) Now if we observe Payload Vs. Launch Site scatter point chart we, that find for the VAFB-SLC launchsite there are no rockets launched for heavy payload mass(greater than 10000). Next, we want to visually check if there are any relationship between success rate and orbit type. data %&gt;% group_by(Orbit) %&gt;% summarise(success = mean(Class)) %&gt;% ggplot(aes(x=Orbit,weight=success,fill=Orbit)) + geom_bar(show.legend=FALSE) Orbits ES-L1, GEO, HEO and SSO have highest success rates followed by VLEO. Finally we would like to explore if there is a relationsship between flight number and orbit type used: data %&gt;% ggplot(mapping = aes(x=FlightNumber,y=Orbit,color=as.factor(Class))) + geom_point(size = 2) + scale_color_hue(&quot;Mission Outcome&quot;, breaks = c(0,1), labels = c(&quot;Failure&quot;,&quot;Success&quot;) ) + labs(x=&quot;FlightNumber&quot;, y=&quot;Orbit&quot;, color=&quot;Outcome&quot;, title=&quot;Orbit Type vs Flight Number&quot;) We see that the successful orbit types correlates with later flight numbers. It is not yet evident if the flights are succesful due to choice of orbit og because of experience. data %&gt;% mutate(Date=year(Date)) %&gt;% group_by(Date) %&gt;% summarise(success = mean(Class)) %&gt;% ggplot(aes(x=Date,y=success)) + geom_line() + scale_x_continuous(breaks = seq(2010,2020),limits=c(2010,2020)) + scale_y_continuous(limit=c(0,1)) + labs(x=&quot;Year&quot;, y=&quot;Success Rate&quot;, title=&quot;Trend in success rate&quot;) We observe that success rate has been steadily increasing since 2013 with the exception of 2018. As noted earlier it is not possible to determine if the increase is related to use of orbit type of mere experience. With this, we conclude the visualization of the dataset. In the next section we take a look at the geospatial data related to the launches. 2.4 Interactive Visual Analytics In this section we will first analyze the geospatial data in the dataset. The spatial data refers primarily to the launch sites. Thus we will have a look at what characterizes the launch sites using an interactive map. In the second part of this section an interactive dashboard to enable further visual EDA of the dataset has been develop using RShiny. 2.4.1 Map data with Leaflet The launch success rate may depend on many factors such as payload mass, orbit type, and so on. It may also depend on the location and proximities of a launch site, i.e., the initial position of rocket trajectories. Finding an optimal location for building a launch site certainly involves many factors and hopefully we could discover some of the factors by analyzing the existing launch site locations. We start by selecting the geospatial data and grouping it by launch site: nasa &lt;- c(29.55968,-95.08310) geodata &lt;- data %&gt;% select(LaunchSite,Latitude,Longitude) %&gt;% group_by(LaunchSite) %&gt;% distinct(LaunchSite,Latitude,Longitude) First let us initiate a map centered at the NASA Johnson Space Center in Houston, Tx, and mark the three launch sites on the map: m &lt;- leaflet(options = leafletOptions(center=nasa,zoom=4)) %&gt;% addTiles() %&gt;% addMarkers(lat=geodata$Latitude,lng=geodata$Longitude,popup = geodata$LaunchSite,label=geodata$LaunchSite) %&gt;% addCircleMarkers(lat=geodata$Latitude,lng=geodata$Longitude,radius=10,color=&quot;#d35400&quot;,fill=TRUE) %&gt;% addMarkers(lat=nasa[1],lng=nasa[2],popup = &quot;NASA Johnson Space Center&quot; ,label=&quot;NASA JSC&quot;) %&gt;% addCircleMarkers(lat=nasa[1],lng=nasa[2],radius=10,color=&quot;#d35400&quot;,fill=TRUE) m Next, let’s try to enhance the map by adding the launch outcomes for each site, and see which sites have high success rates. For this we need the launch data with outcome included. To facilitate colouring icons later we add a Colour column, where Colour is ‘green’ is Class is 1 and Colour is ‘red’ if Class is 0: launchdata &lt;- data %&gt;% select(LaunchSite,Latitude,Longitude,Class) %&gt;% add_column(Color = NA) %&gt;% mutate(Color = ifelse(Class ==1,&#39;green&#39;,&#39;red&#39;)) Now we build a map with each launch clustered at the launch sites and each launch icon coloured by the outcome class. In the top we add a reading of longitude and lattitude for mouse position: m &lt;- leaflet(options = leafletOptions(center=nasa,zoom=4)) %&gt;% addTiles() %&gt;% addAwesomeMarkers(lat=launchdata$Latitude,lng=launchdata$Longitude,popup = launchdata$LaunchSite,label=launchdata$LaunchSite, icon=makeAwesomeIcon(icon = &quot;plane&quot;, library = &quot;ion&quot;, markerColor=&#39;white&#39;,iconColor = launchdata$Color),clusterOptions = markerClusterOptions()) %&gt;% addCircleMarkers(lat=geodata$Latitude,lng=geodata$Longitude,radius=10,color=&quot;#d35400&quot;,fill=TRUE) %&gt;% addMarkers(lat=nasa[1],lng=nasa[2],popup = &quot;NASA Johnson Space Center&quot; ,label=&quot;NASA JSC&quot;) %&gt;% addCircleMarkers(lat=nasa[1],lng=nasa[2],radius=10,color=&quot;#d35400&quot;,fill=TRUE) %&gt;% addMouseCoordinates() m It seems that most successful launches were made from KSC LC-39A. Now we will read coordinates for the closest railway, highway, coastline and urban structure relative to this launchfacility: structure &lt;- tibble(&#39;Location&#39;=character(),&#39;Lat&#39;=numeric(),&#39;Lng&#39;=numeric()) %&gt;% add_row(&#39;Location&#39;=&#39;Coast&#39;,&#39;Lng&#39;=-80.59791,&#39;Lat&#39;=28.61245) %&gt;% add_row(&#39;Location&#39;=&#39;Urban&#39;,&#39;Lng&#39;=-80.81000,&#39;Lat&#39;=28.61155) %&gt;% add_row(&#39;Location&#39;=&#39;Highway&#39;,&#39;Lng&#39;=-80.66407,&#39;Lat&#39;=28.52600) %&gt;% add_row(&#39;Location&#39;=&#39;Railway&#39;,&#39;Lng&#39;=-80.61646,&#39;Lat&#39;=28.62802) knitr::kable(structure,format=&#39;html&#39;,align=&#39;c&#39;) Location Lat Lng Coast 28.61245 -80.59791 Urban 28.61155 -80.81000 Highway 28.52600 -80.66407 Railway 28.62802 -80.61646 To help calculating distances between positions of the map we define a function of latitude and longitude coordinates: distance_function &lt;- function(lat1,lng1,lat2,lng2){ R &lt;- 6373.0 lat1 &lt;- deg2rad(lat1) lng1 &lt;- deg2rad(lng1) lat2 &lt;- deg2rad(lat2) lng2 &lt;- deg2rad(lng2) dlon &lt;- lng2 - lng1 dlat &lt;- lat2 - lat1 a=sin(dlat/2)^2+cos(lat1)*cos(lat2)*sin(dlon/2)^2 c=2*atan2(sqrt(a),sqrt(1-a)) dist = R*c return(dist) } distance_to_launch &lt;- function(lat,lng){ distance_function(lat,lng,geodata$Latitude[3],geodata$Longitude[3]) } We now add a column to the structure data with the distance from each structure to launch site KSC LC-39A structure &lt;- structure %&gt;% add_column(&#39;Distance&#39;=NA) %&gt;% mutate(&#39;Distance&#39;=map2(structure$Lat,structure$Lng,~distance_to_launch(.x,.y))) We now mark the coast, highway, railway and urban structures to the map together with the distance to KSC LC-39A: linedata &lt;- tibble(&#39;lat&#39;=numeric(),&#39;lng&#39;=numeric()) %&gt;% add_row(&#39;lat&#39;=geodata$Latitude[3],&#39;lng&#39;=geodata$Longitude[3]) %&gt;% add_row(&#39;lat&#39;=structure$Lat[1],&#39;lng&#39;=structure$Lng[1]) %&gt;% add_row(&#39;lat&#39;=geodata$Latitude[3],&#39;lng&#39;=geodata$Longitude[3]) %&gt;% add_row(&#39;lat&#39;=structure$Lat[2],&#39;lng&#39;=structure$Lng[2]) %&gt;% add_row(&#39;lat&#39;=geodata$Latitude[3],&#39;lng&#39;=geodata$Longitude[3]) %&gt;% add_row(&#39;lat&#39;=structure$Lat[3],&#39;lng&#39;=structure$Lng[3]) %&gt;% add_row(&#39;lat&#39;=geodata$Latitude[3],&#39;lng&#39;=geodata$Longitude[3]) %&gt;% add_row(&#39;lat&#39;=structure$Lat[4],&#39;lng&#39;=structure$Lng[4]) %&gt;% add_row(&#39;lat&#39;=geodata$Latitude[3],&#39;lng&#39;=geodata$Longitude[3]) m2 &lt;- leaflet(options = leafletOptions(center=c(28.61377,-80.69307),zoom=11)) %&gt;% addTiles() %&gt;% addMarkers(lat=geodata$Latitude[3],lng=geodata$Longitude[3],popup=geodata$LaunchSite[3],label=geodata$LaunchSite[3]) %&gt;% addMarkers(lat=structure$Lat,lng=structure$Lng,label = paste0(&quot;Distance from &quot;,structure$Location,&quot; to KSC LC-39A is &quot;,sprintf(structure$Distance, fmt=&#39;%.2f&#39;),&quot; km&quot;)) %&gt;% addPolylines(lng=linedata$lng,lat=linedata$lat) m2 2.4.2 Dashboard for further visual EDA In this section we will develop a dashboard to further investigate features of the launchsites. The dashboard will consist of a piechart to show success rates for the launch sites with a dropdown menu to select launch sites. Moreover we will have a scatterplot of success class vs payload mass for all or individual launch sites together with a slider to select payload mass range. launchsites &lt;- c(geodata$LaunchSite,&#39;All&#39;) ui &lt;- fluidPage( titlePanel(&quot;SpaceX Launch Records Dashboard&quot;), fluidRow(column(12, selectInput(&quot;site&quot;,&quot;Select Launch Site&quot;,launchsites,selected=&#39;All&#39;) )), fluidRow(column(12, plotOutput(&quot;piechart&quot;) )), fluidRow(column(2,), column(8, sliderInput(&quot;payload&quot;,&quot;Select Payload Mass Range&quot;, 0,10000,c(0,10000),step=1000,width=&#39;100%&#39;) ), column(2,)), fluidRow(column(12, plotOutput(&quot;scatterchart&quot;) )) ) server &lt;- function(input, output, session) { site_select &lt;- reactive(input$site) min_payload &lt;- reactive(input$payload[1]) max_payload &lt;- reactive(input$payload[2]) output$piechart &lt;- renderPlot({ if (site_select() == &#39;All&#39;) { data %&gt;% ggplot(mapping=aes(x=&quot;&quot;,y=Class,fill=factor(LaunchSite))) + geom_bar(stat=&quot;identity&quot;,width=1) + coord_polar(theta=&quot;y&quot;) + labs(x=&quot;&quot;,y=&quot;&quot;,title=&quot;Distribution of successful launches on launch sites&quot;,fill=&quot;Launch Site&quot;) + theme(axis.text=element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) } else { data %&gt;% filter(LaunchSite == site_select()) %&gt;% group_by(Class) %&gt;% ggplot(mapping = aes(x=&quot;&quot;,y=sum(Class),fill=factor(Class))) + geom_bar(stat=&quot;identity&quot;,width = 1) + coord_polar(theta=&quot;y&quot;,start=0) + labs(x=&quot;&quot;,y=&quot;&quot;,title=glue(&quot;Distribution og successful and unsuccessful launches at {site_select()}&quot;)) + scale_fill_discrete(&quot;Outcome&quot;,labels=c(&quot;Failure&quot;,&quot;Success&quot;)) + theme(axis.text=element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) } }) output$scatterchart &lt;- renderPlot({ if (site_select() == &#39;All&#39;) { data %&gt;% filter(PayloadMass &gt;= min_payload(),PayloadMass &lt;= max_payload()) %&gt;% ggplot(mapping=aes(x=PayloadMass,y=Class,color=LaunchSite)) + geom_point(size=2) + labs(x=&quot;Payload Mass (kg)&quot;,y=&quot;Outcome Class&quot;,title=&quot;Outcome Class vs. Payload Mass (kg) for all launch sites&quot;) } else { data %&gt;% filter(LaunchSite == site_select()) %&gt;% filter(PayloadMass &gt;= min_payload(),PayloadMass &lt;= max_payload()) %&gt;% ggplot(mapping=aes(x=PayloadMass,y=Class,color=LaunchSite)) + geom_point(size=2) + labs(x=&quot;Payload Mass (kg)&quot;, y=&quot;Outcome Class&quot;,title=glue(&quot;Outcome Class vs. Payload Mass (kg) for {site_select()}&quot;)) } }) } shinyApp(ui, server) include_app(&quot;https://bjarnelp.shinyapps.io/capstoneproject/&quot;, height=&quot;500px&quot;) 2.5 Machine Learning Models Space X advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because Space X can reuse the first stage. Therefore if we can determine if the first stage will land, we can determine the cost of a launch. This information can be used if an alternate company wants to bid against space X for a rocket launch. In this lab, you will create a machine learning pipeline to predict if the first stage will land given the data from the preceding labs. In this chapter we will split the data into a training set and a test set and then fit logistic regression, SVM, kNN and decision tree classifiers to the test set. Best parameters will be determined using 10-fold cross validation grid search. Performance of each model is calculated using the test set, visualized using a tile plot of the confusion matrix as well as determined using the F1-score. For the classication models, the outcome variable must be a factor. Thus we start by converting the Class variable to a factor: data &lt;- data %&gt;% mutate(Class = as_factor(Class)) First we will create a 80/20 training/test split using random sampling: set.seed(123) data_split &lt;- initial_split(data, prop=0.8) train_data &lt;- training(data_split) test_data &lt;- testing(data_split) To facilitate 10-fold cross validation, we set up the folds: set.seed(222) cv_folds &lt;- vfold_cv(train_data,v=5) Next we will make a preprocessing and formula recipe, where Flightnumber, Date, Serial, Longitude and Latitude are retained as ID variables. Factors are one-hot-encoded and numeric variables are normalized to have mean 0 and standard deviation 1: spacex_rec &lt;- recipe(Class ~., data=train_data) %&gt;% update_role(BoosterVersion,Outcome,FlightNumber,Date,Serial, Longitude,Latitude, new_role=&quot;ID&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) 2.5.1 Logistic regression We initialize a logistic regression object using glmnet as engine. The penalty and mixture (L1 vs. L2 penalty) are set up as tunable parameters: logit_reg &lt;- logistic_reg(penalty=tune(),mixture=tune()) %&gt;% set_engine(&quot;glmnet&quot;) Next we set up the grid for the grid search. We use five levels of penalty and two leves of mixture (0 and 1): logit_grid &lt;- grid_regular(penalty(),mixture(),levels=c(5,2)) Finally we set up a workflow using the model and the recipe: logit_wf &lt;- workflow() %&gt;% add_model(logit_reg) %&gt;% add_recipe(spacex_rec) Now let us do the tuning: Now let us have a look at the results autoplot(logit_res) It seems that in general Lasso regression is better than Ridge regression and that regulization should be between 0.001 and 0.01. We can now perform the final fit on the train data and evaluate using the test data: logit_best &lt;- logit_res %&gt;% select_best(&quot;accuracy&quot;) logit_final_wf &lt;- logit_wf %&gt;% finalize_workflow(logit_best) logit_final &lt;- logit_final_wf %&gt;% last_fit(data_split) First let us retrieve the predictions and have a look at the confusion matrix logit_cm &lt;- logit_final %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(logit_cm, type=&quot;heatmap&quot;)) For the logistic regression we get a F1-score of 0.846. 2.5.2 Support Vector Machines (SVM) We can recycle the recipe from logistic regression, and thus we begin be instantiate the SVM model: svm_lin &lt;- svm_linear(cost=tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;kernlab&quot;, scaled=TRUE) Next we set up the grid for the grid search. We use three levels of cost and two levels of margin: svm_lin_grid &lt;- grid_regular(cost(),levels=5) Finally we set up a workflow using the model and the recipe and do the tuning: Now let us have a look at the results autoplot(svm_lin_res) We find that a cost of 32.0 gives the best accuracy, and we can now perform the final fit on the train data and evaluate using the test data: svm_lin_best &lt;- svm_lin_res %&gt;% select_best(&quot;accuracy&quot;) svm_lin_final_wf &lt;- svm_lin_wf %&gt;% finalize_workflow(svm_lin_best) svm_lin_final &lt;- svm_lin_final_wf %&gt;% last_fit(data_split) Finally let us retrieve the predictions and have a look at the confusion matrix svm_lin_cm &lt;- svm_lin_final %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(svm_lin_cm, type=&quot;heatmap&quot;)) We find a F1-score of 0.741, which is significantly lower than for the logistic regression model. We now repeat the process for polynomial af radial basis kernels with the exception that the polynomial kernel introduces a new tunable parameter for the polynomial degree and the radial basis kernel introduces the tunable sigma parameter. Let us have a look at the results for the polynomial kernel: autoplot(svm_pol_res) We see that a linear kernel performs best, so we will not deal anymore with the polynomial kernel. For the radial basis kernel: autoplot(svm_radial_res) We find that a gives the best accuracy, and we can now perform the final fit on the train data and evaluate using the test data: svm_radial_best &lt;- svm_radial_res %&gt;% select_best(&quot;accuracy&quot;) svm_radial_final_wf &lt;- svm_radial_wf %&gt;% finalize_workflow(svm_radial_best) svm_radial_final &lt;- svm_radial_final_wf %&gt;% last_fit(data_split) Finally let us retrieve the predictions and have a look at the confusion matrix svm_radial_cm &lt;- svm_radial_final %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(svm_radial_cm, type=&quot;heatmap&quot;)) The F1-score for the radial basis function kernel is 0.815, which is better than for the linear kernel but less than for the logistic regression. 2.5.3 kNN classifier We can recycle the recipe from earlier, and thus we begin be instantiate the kNN model: knn_mod &lt;- nearest_neighbor(neighbors= tune(), weight_func=tune(), dist_power = tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;kknn&quot;) Next we set up the grid for the grid search. We use three levels of cost and two levels of margin: knn_grid &lt;- grid_regular(neighbors(),weight_func(),dist_power(),levels=c(10,9,4)) Finally we set up a workflow using the model and the recipe and do the tuning: Now let us have a look at the results autoplot(knn_res) Based on accuracy it seems the best choise would be N=5, rectangular weight function and L1-distance. knn_best &lt;- knn_res %&gt;% select_best(&quot;accuracy&quot;) knn_final_wf &lt;- knn_wf %&gt;% finalize_workflow(knn_best) knn_final &lt;- knn_final_wf %&gt;% last_fit(data_split) Finally let us retrieve the predictions and have a look at the confusion matrix knn_cm &lt;- knn_final %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(knn_cm, type=&quot;heatmap&quot;)) We get a F1-score of 0,846 on par with the logistic regression model. 2.5.4 Decision tree classifier For the decision tree classifier we will reuse the recipe from earlier, though we don’t need to encode factors and standardize numerical features for this model. Let’s start by instatiating the model: tree &lt;- decision_tree(tree_depth = tune(), min_n = tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;rpart&quot;) Next we set up the grid for the grid search. We use three levels of cost and two levels of margin: tree_grid &lt;- grid_regular(min_n(),tree_depth(),levels=c(5,10)) Finally we set up a workflow using the model and the recipe and do the tuning: Now let us have a look at the results autoplot(tree_res) It seems that we get the best decision tree with a rather small tree and a rather big minimal node size. Let us construct the best tree and evaluate the performance: tree_best &lt;- tree_res %&gt;% select_best(&quot;accuracy&quot;) tree_final_wf &lt;- tree_wf %&gt;% finalize_workflow(tree_best) tree_final &lt;- tree_final_wf %&gt;% last_fit(data_split) Finally let us retrieve the predictions and have a look at the confusion matrix tree_cm &lt;- tree_final %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(tree_cm, type=&quot;heatmap&quot;)) With an F1-score of 0,833, it performs slightly worse than the logistic regression and SVM. As noted in the beginning, the decision tree classifier doesn’t need as much preprocessing as some of the other classifiers. So let’s end this chapter with a decision tree based on less preprocessing. First let us define a new recipe: tree_rec &lt;- recipe(Class ~., data=train_data) %&gt;% update_role(BoosterVersion,Outcome,FlightNumber,Date,Serial, Longitude,Latitude, new_role=&quot;ID&quot;) Next we define a new workflow based on the new recipe: Now let us have a look at the results to decide best parameters: autoplot(tree_res2) Agin, it seems that we get the best decision tree with a rather small tree and a rather big minimal node size. Let us construct the best tree and evaluate the performance: tree_best2 &lt;- tree_res2 %&gt;% select_best(&quot;accuracy&quot;) tree_final_wf2 &lt;- tree_wf2 %&gt;% finalize_workflow(tree_best2) tree_final2 &lt;- tree_final_wf2 %&gt;% last_fit(data_split) Finally let us retrieve the predictions and have a look at the confusion matrix tree_cm2 &lt;- tree_final2 %&gt;% collect_predictions() %&gt;% conf_mat(.pred_class,Class) print(autoplot(tree_cm2, type=&quot;heatmap&quot;)) We see that we get exactly the same accuracy as we did with the preprocessing. 2.5.5 Outro In this section we have fitted several models to the spaceX dataset. The best model turned out to be an L2-penalized logistic regression. Let us have a look at the predictors for the model: logit_fitted &lt;- logit_final_wf %&gt;% fit(data) tidy(logit_fitted) %&gt;% filter(estimate !=0) ## # A tibble: 19 × 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.40 0.00316 ## 2 PayloadMass 0.0122 0.00316 ## 3 Flights -3.37 0.00316 ## 4 Reused 1.83 0.00316 ## 5 Legs 3.56 0.00316 ## 6 Block -0.596 0.00316 ## 7 ReusedCount 8.73 0.00316 ## 8 Orbit_GTO -0.438 0.00316 ## 9 Orbit_HEO 1.25 0.00316 ## 10 Orbit_ISS -0.910 0.00316 ## 11 Orbit_LEO 0.847 0.00316 ## 12 Orbit_PO -0.253 0.00316 ## 13 Orbit_SO -1.54 0.00316 ## 14 Orbit_VLEO -2.42 0.00316 ## 15 LaunchSite_KSC.LC.39A 1.42 0.00316 ## 16 LandingPad_X5e9e3032383ecb6bb234e7ca -0.301 0.00316 ## 17 LandingPad_X5e9e3032383ecb761634e7cb -3.94 0.00316 ## 18 LandingPad_X5e9e3033383ecbb9e534e7cc -0.462 0.00316 ## 19 LandingPad_FALSE. -0.546 0.00316 We see that the features most important for a successful outcome is if the first stage has been reused, and how many times it has been reused, if legs were used and the use of orbit HEO and LEO and launch site KSC LC-39A. These observations are in general in line with the findings in the EDA section. "],["titanic---studies-on-classification.html", "Chapter 3 Titanic - studies on classification", " Chapter 3 Titanic - studies on classification In this chapter we will turn our focus til classification techniques. The dataset used will be the Kaggle dataset Titanic. The first section will focus on data collection and data wrangling. The second section has its focus on EDA. In the third and last section we will study the following classification models and how to optimize them for the dataset: Logistic Regression, kNN, SVM, XGBoost and lightGBM. The evaluation metric will be the Kaggle score for each model. "],["ames-housing-data---regression-methods.html", "Chapter 4 Ames Housing Data - regression methods", " Chapter 4 Ames Housing Data - regression methods "],["store-sales---time-series-prediction.html", "Chapter 5 Store Sales - Time Series Prediction", " Chapter 5 Store Sales - Time Series Prediction "],["petals-to-the-metal---image-classification-using-tensorflow.html", "Chapter 6 Petals to the Metal - Image Classification using tensorflow", " Chapter 6 Petals to the Metal - Image Classification using tensorflow "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
